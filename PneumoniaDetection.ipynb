{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Untitled5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AI-Junction/Pneumonia-Detection/blob/master/PneumoniaDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "0NoF022TreRu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Check memory allocation to this sesssion"
      ]
    },
    {
      "metadata": {
        "id": "HiC2mY5E57ad",
        "colab_type": "code",
        "outputId": "a41c787a-ba27-4b79-d07b-bebf8967d0d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "cell_type": "code",
      "source": [
        "!df -h"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay         359G   25G  316G   8% /\n",
            "tmpfs           6.4G     0  6.4G   0% /dev\n",
            "tmpfs           6.4G     0  6.4G   0% /sys/fs/cgroup\n",
            "/dev/sda1       365G   28G  338G   8% /opt/bin\n",
            "tmpfs           6.4G  8.0K  6.4G   1% /var/colab\n",
            "shm             6.0G  4.0K  6.0G   1% /dev/shm\n",
            "tmpfs           6.4G     0  6.4G   0% /sys/firmware\n",
            "drive           100G   19G   82G  19% /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lTGfs5-Q27X_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Confirm TensorFlow can see the GPU\n",
        "\n",
        "Simply select \"GPU\" in the Accelerator drop-down in Notebook Settings (either through the Edit menu or the command palette at cmd/ctrl-shift-P)."
      ]
    },
    {
      "metadata": {
        "id": "BN9O5cVk2uJH",
        "colab_type": "code",
        "outputId": "c8b678e8-aa58-42a6-dd25-8b63154c1b94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "o40Dy-h431QS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Check RAM allocation in current session"
      ]
    },
    {
      "metadata": {
        "id": "dm9C91QJ30RX",
        "colab_type": "code",
        "outputId": "11d2a739-a3e8-41a7-ed61-70be24264f95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "cell_type": "code",
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Running setup.py bdist_wheel for gputil ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.2 GB  | Proc size: 1.1 GB\n",
            "GPU RAM Free: 11064MB | Used: 377MB | Util   3% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B-PFAjmv2yzZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Observe TensorFlow speedup on GPU relative to CPU\n",
        "\n",
        "**** Uncomment below cell if needed"
      ]
    },
    {
      "metadata": {
        "id": "bzbVk-cP2x9c",
        "colab_type": "code",
        "outputId": "d2b26123-a352-4269-d652-4f22ba6cc90b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "# See https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "with tf.device('/cpu:0'):\n",
        "  random_image_cpu = tf.random_normal((100, 100, 100, 3))\n",
        "  net_cpu = tf.layers.conv2d(random_image_cpu, 32, 7)\n",
        "  net_cpu = tf.reduce_sum(net_cpu)\n",
        "\n",
        "with tf.device('/gpu:0'):\n",
        "  random_image_gpu = tf.random_normal((100, 100, 100, 3))\n",
        "  net_gpu = tf.layers.conv2d(random_image_gpu, 32, 7)\n",
        "  net_gpu = tf.reduce_sum(net_gpu)\n",
        "\n",
        "sess = tf.Session(config=config)\n",
        "\n",
        "# Test execution once to detect errors early.\n",
        "try:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "except tf.errors.InvalidArgumentError:\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise\n",
        "\n",
        "def cpu():\n",
        "  sess.run(net_cpu)\n",
        "  \n",
        "def gpu():\n",
        "  sess.run(net_gpu)\n",
        "  \n",
        "# Runs the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))\n",
        "\n",
        "sess.close()\n",
        "\n",
        "'''"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nimport tensorflow as tf\\nimport timeit\\n\\n# See https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth\\nconfig = tf.ConfigProto()\\nconfig.gpu_options.allow_growth = True\\n\\nwith tf.device(\\'/cpu:0\\'):\\n  random_image_cpu = tf.random_normal((100, 100, 100, 3))\\n  net_cpu = tf.layers.conv2d(random_image_cpu, 32, 7)\\n  net_cpu = tf.reduce_sum(net_cpu)\\n\\nwith tf.device(\\'/gpu:0\\'):\\n  random_image_gpu = tf.random_normal((100, 100, 100, 3))\\n  net_gpu = tf.layers.conv2d(random_image_gpu, 32, 7)\\n  net_gpu = tf.reduce_sum(net_gpu)\\n\\nsess = tf.Session(config=config)\\n\\n# Test execution once to detect errors early.\\ntry:\\n  sess.run(tf.global_variables_initializer())\\nexcept tf.errors.InvalidArgumentError:\\n  print(\\n      \\'\\n\\nThis error most likely means that this notebook is not \\'\\n      \\'configured to use a GPU.  Change this in Notebook Settings via the \\'\\n      \\'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n\\')\\n  raise\\n\\ndef cpu():\\n  sess.run(net_cpu)\\n  \\ndef gpu():\\n  sess.run(net_gpu)\\n  \\n# Runs the op several times.\\nprint(\\'Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images \\'\\n      \\'(batch x height x width x channel). Sum of ten runs.\\')\\nprint(\\'CPU (s):\\')\\ncpu_time = timeit.timeit(\\'cpu()\\', number=10, setup=\"from __main__ import cpu\")\\nprint(cpu_time)\\nprint(\\'GPU (s):\\')\\ngpu_time = timeit.timeit(\\'gpu()\\', number=10, setup=\"from __main__ import gpu\")\\nprint(gpu_time)\\nprint(\\'GPU speedup over CPU: {}x\\'.format(int(cpu_time/gpu_time)))\\n\\nsess.close()\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "metadata": {
        "id": "JQ5kplL6sbfy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Check total memory allocation\n",
        "\n",
        "*** Uncomment below cell only if needed"
      ]
    },
    {
      "metadata": {
        "id": "kcFqYxzH9z7O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!cat /proc/meminfo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dWOpFaOIsl6T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Check versions of various libraries installed in this session of colaboratory\n",
        "\n",
        "**** Uncomment below cell only if needed"
      ]
    },
    {
      "metadata": {
        "id": "HbIRMWHURzeQ",
        "colab_type": "code",
        "outputId": "1ff6b9b0-1da4-4bbb-a864-969d06221152",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "import sys #access to system parameters https://docs.python.org/3/library/sys.html\n",
        "print(\"Python version: {}\". format(sys.version))\n",
        "\n",
        "import pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\n",
        "print(\"pandas version: {}\". format(pd.__version__))\n",
        "\n",
        "import matplotlib #collection of functions for scientific and publication-ready visualization\n",
        "print(\"matplotlib version: {}\". format(matplotlib.__version__))\n",
        "\n",
        "import numpy as np #foundational package for scientific computing\n",
        "print(\"NumPy version: {}\". format(np.__version__))\n",
        "\n",
        "import scipy as sp #collection of functions for scientific computing and advance mathematics\n",
        "print(\"SciPy version: {}\". format(sp.__version__)) \n",
        "\n",
        "import IPython\n",
        "from IPython import display #pretty printing of dataframes in Jupyter notebook\n",
        "print(\"IPython version: {}\". format(IPython.__version__)) \n",
        "\n",
        "import sklearn #collection of machine learning algorithms\n",
        "print(\"scikit-learn version: {}\". format(sklearn.__version__))\n",
        "\n",
        "#misc libraries\n",
        "import random\n",
        "import time\n",
        "\n",
        "'''"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nimport sys #access to system parameters https://docs.python.org/3/library/sys.html\\nprint(\"Python version: {}\". format(sys.version))\\n\\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\\nprint(\"pandas version: {}\". format(pd.__version__))\\n\\nimport matplotlib #collection of functions for scientific and publication-ready visualization\\nprint(\"matplotlib version: {}\". format(matplotlib.__version__))\\n\\nimport numpy as np #foundational package for scientific computing\\nprint(\"NumPy version: {}\". format(np.__version__))\\n\\nimport scipy as sp #collection of functions for scientific computing and advance mathematics\\nprint(\"SciPy version: {}\". format(sp.__version__)) \\n\\nimport IPython\\nfrom IPython import display #pretty printing of dataframes in Jupyter notebook\\nprint(\"IPython version: {}\". format(IPython.__version__)) \\n\\nimport sklearn #collection of machine learning algorithms\\nprint(\"scikit-learn version: {}\". format(sklearn.__version__))\\n\\n#misc libraries\\nimport random\\nimport time\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "metadata": {
        "id": "Kw8e1sxsWAYO",
        "colab_type": "code",
        "outputId": "0dd33973-725b-4199-fd81-c5712fc6f277",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "!git clone https://gist.github.com/dc7e60aa487430ea704a8cb3f2c5d6a6.git /tmp/colab_util_repo\n",
        "!mv /tmp/colab_util_repo/colab_util.py colab_util.py \n",
        "!rm -r /tmp/colab_util_repo"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '/tmp/colab_util_repo'...\n",
            "remote: Enumerating objects: 40, done.\u001b[K\n",
            "remote: Total 40 (delta 0), reused 0 (delta 0), pack-reused 40\u001b[K\n",
            "Unpacking objects: 100% (40/40), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CjBOuMRy3gnP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!ls -l /tmp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A7hM__AsVyTA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from colab_util import *\n",
        "drive_handler = GoogleDriveHandler()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ARKd_w1TpYUM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dUz-iLKIitkt",
        "colab_type": "code",
        "outputId": "500bb6d7-901c-4612-e678-c793ab366fff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4JRbSL-ToWIV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "6a45705e-468f-44d2-a193-4dc7a6b1311a"
      },
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.1.1)\n",
            "Requirement already satisfied: urllib3<1.23.0,>=1.15 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.22)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2018.11.29)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.0.1)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: Unidecode>=0.04.16 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.0.23)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "De3f_YB8obZP",
        "colab_type": "code",
        "outputId": "ad914fc0-6370-4c3c-ef9c-3947773d4f83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "import io, os\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "drive_service = build('drive', 'v3')\n",
        "results = drive_service.files().list(\n",
        "        q=\"name = 'kaggle.json'\", fields=\"files(id)\").execute()\n",
        "kaggle_api_key = results.get('files', [])\n",
        "\n",
        "#filename = \"/content/.kaggle/kaggle.json\"\n",
        "#filename = \"/.kaggle/kaggle.json\"\n",
        "filename = \"kaggle.json\"\n",
        "#os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "\n",
        "request = drive_service.files().get_media(fileId=kaggle_api_key[0]['id'])\n",
        "fh = io.FileIO(filename, 'wb')\n",
        "downloader = MediaIoBaseDownload(fh, request)\n",
        "done = False\n",
        "while done is False:\n",
        "    status, done = downloader.next_chunk()\n",
        "    print(\"Download %d%%.\" % int(status.progress() * 100))\n",
        "os.chmod(filename, 600)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Download 100%.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_QJZsCT4o3A5",
        "colab_type": "code",
        "outputId": "d6823671-664a-489c-a2d5-457c1c36f760",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!ls -lha kaggle.json\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---x-wx--T 1 root root 65 Jan 23 15:17 kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yxb8N3E3o8FN",
        "colab_type": "code",
        "outputId": "4dd9cf87-49c4-41c7-8772-767e0118f9be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "os.path.isdir(\"/root/.kaggle\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "9uUUcDqhpAuJ",
        "colab_type": "code",
        "outputId": "a2f8c38b-12f1-498d-bfe8-70f6034cf011",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "dir_kaggle = \"/root/.kaggle\"\n",
        "if not os.path.isdir(dir_kaggle):\n",
        "  !mkdir -p ~/.kaggle  \n",
        "!cp kaggle.json ~/.kaggle/\n",
        "os.path.isdir(dir_kaggle)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "_ErgTi5tpFFh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IS_LrIG9B-v5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Create training and test data in this allocated session of colaboratory"
      ]
    },
    {
      "metadata": {
        "id": "z0lIooaisXL6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir '/content/Pneumonia-Detection'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6Rwv2qgLtleC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "ec7260a7-5b16-4374-c984-1a405b530b56"
      },
      "cell_type": "code",
      "source": [
        "!kaggle competitions files -c rsna-pneumonia-detection-challenge"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "name                                  size  creationDate         \n",
            "-----------------------------------  -----  -------------------  \n",
            "stage_2_detailed_class_info.csv        2MB  2018-10-25 21:57:44  \n",
            "stage_2_sample_submission.csv        155KB  2018-10-25 21:57:44  \n",
            "stage_2_train_labels.csv               1MB  2018-10-25 21:57:45  \n",
            "stage_2_test_images.zip              377MB  2018-10-25 21:57:54  \n",
            "stage_2_train_images.zip               3GB  2018-10-25 21:59:27  \n",
            "GCP Credits Request Link - RSNA.txt    55B  2018-10-25 22:02:52  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7ZPBirExtfT2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nJIxGpxNtxvN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "4980a1cc-202f-4947-8458-fa09359c19b3"
      },
      "cell_type": "code",
      "source": [
        "if not os.path.isfile(\"/content/Pneumonia-Detection/stage_2_test_images.zip\"):\n",
        "  !kaggle competitions download -c rsna-pneumonia-detection-challenge -f stage_2_detailed_class_info.csv -p \"/content/Pneumonia-Detection/\"\n",
        "  !kaggle competitions download -c rsna-pneumonia-detection-challenge -f stage_2_sample_submission.csv -p \"/content/Pneumonia-Detection/\"\n",
        "  !kaggle competitions download -c rsna-pneumonia-detection-challenge -f stage_2_train_labels.csv -p \"/content/Pneumonia-Detection/\"\n",
        "  !kaggle competitions download -c rsna-pneumonia-detection-challenge -f stage_2_test_images.zip -p \"/content/Pneumonia-Detection/\"\n",
        "  !kaggle competitions download -c rsna-pneumonia-detection-challenge -f stage_2_train_images.zip -p \"/content/Pneumonia-Detection/\"\n",
        "  !kaggle competitions download -c rsna-pneumonia-detection-challenge -f GCP Credits Request Link - RSNA.txt -p \"/content/Pneumonia-Detection/\"\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading stage_2_detailed_class_info.csv.zip to /content/Pneumonia-Detection\n",
            "\r  0% 0.00/583k [00:00<?, ?B/s]\n",
            "100% 583k/583k [00:00<00:00, 38.0MB/s]\n",
            "Downloading stage_2_sample_submission.csv to /content/Pneumonia-Detection\n",
            "  0% 0.00/155k [00:00<?, ?B/s]\n",
            "100% 155k/155k [00:00<00:00, 56.4MB/s]\n",
            "Downloading stage_2_train_labels.csv.zip to /content/Pneumonia-Detection\n",
            "  0% 0.00/661k [00:00<?, ?B/s]\n",
            "100% 661k/661k [00:00<00:00, 43.1MB/s]\n",
            "Downloading stage_2_test_images.zip to /content/Pneumonia-Detection\n",
            " 99% 374M/377M [00:08<00:00, 38.5MB/s]\n",
            "100% 377M/377M [00:08<00:00, 47.4MB/s]\n",
            "Downloading stage_2_train_images.zip to /content/Pneumonia-Detection\n",
            "100% 3.29G/3.29G [01:27<00:00, 33.0MB/s]\n",
            "100% 3.29G/3.29G [01:27<00:00, 40.3MB/s]\n",
            "usage: kaggle [-h] [-v] {competitions,c,datasets,d,kernels,k,config} ...\n",
            "kaggle: error: unrecognized arguments: Request Link - RSNA.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8kAdMB0du66t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "a9c7b6ce-0f20-4f79-d6d6-3dbd68e4ff5b"
      },
      "cell_type": "code",
      "source": [
        "!pip install pydicom"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pydicom\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/88/d3c419ab2e753e7651510882a53219373e78fb55294cb247dffd3934ea55/pydicom-1.2.2-py2.py3-none-any.whl (7.0MB)\n",
            "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.0MB 7.0MB/s \n",
            "\u001b[?25hInstalling collected packages: pydicom\n",
            "Successfully installed pydicom-1.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mZuQZyCzwerD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if not os.path.isdir(\"/content/Pneumonia-Detection/train\"):\n",
        "  !mkdir \"/content/Pneumonia-Detection/train\"\n",
        "  !unzip \"/content/Pneumonia-Detection/stage_2_train_images.zip\" -d \"/content/Pneumonia-Detection/train\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l0eukYYHyDI9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if not os.path.isdir(\"/content/Pneumonia-Detection/test\"):\n",
        "  !mkdir \"/content/Pneumonia-Detection/test\"\n",
        "  !unzip \"/content/Pneumonia-Detection/stage_2_test_images.zip\" -d \"/content/Pneumonia-Detection/test\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SLUNuGt2xKdx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "66d5270e-c869-4721-f2b0-79690b686688"
      },
      "cell_type": "code",
      "source": [
        "if not os.path.isfile(\"/content/Pneumonia-Detection/stage_2_detailed_class_info.csv\"):\n",
        "  !unzip \"/content/Pneumonia-Detection/stage_2_detailed_class_info.csv.zip\" -d \"/content/Pneumonia-Detection\"\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/Pneumonia-Detection/stage_2_detailed_class_info.csv.zip\n",
            "  inflating: /content/Pneumonia-Detection/stage_2_detailed_class_info.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aBhjRWXdT36e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e29a2ae5-87e8-4bf8-e79d-0b9b7f55d47e"
      },
      "cell_type": "code",
      "source": [
        "if not os.path.isfile(\"/content/Pneumonia-Detection/stage_2_train_labels.csv\"):\n",
        "  !unzip \"/content/Pneumonia-Detection/stage_2_train_labels.csv.zip\" -d \"/content/Pneumonia-Detection\"\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/Pneumonia-Detection/stage_2_train_labels.csv.zip\n",
            "  inflating: /content/Pneumonia-Detection/stage_2_train_labels.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "h78KKGWCurpq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import random\n",
        "import pydicom\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from skimage import io\n",
        "from skimage import measure\n",
        "from skimage.transform import resize\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.patches as patches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G1fe085fe6we",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_train_labels = pd.read_csv('/content/Pneumonia-Detection/stage_2_train_labels.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4mJhTXhzfB-V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "02f493b5-e938-4d0f-8e9c-80487bfa64bd"
      },
      "cell_type": "code",
      "source": [
        "print(df_train_labels.shape)\n",
        "print(df_train_labels.columns)\n",
        "print(df_train_labels.head())"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30227, 6)\n",
            "Index(['patientId', 'x', 'y', 'width', 'height', 'Target'], dtype='object')\n",
            "                              patientId      x      y  width  height  Target\n",
            "0  0004cfab-14fd-4e49-80ba-63a80b6bddd6    NaN    NaN    NaN     NaN       0\n",
            "1  00313ee0-9eaa-42f4-b0ab-c148ed3241cd    NaN    NaN    NaN     NaN       0\n",
            "2  00322d4d-1c29-4943-afc9-b6754be640eb    NaN    NaN    NaN     NaN       0\n",
            "3  003d8fa0-6bf1-40ed-b54c-ac657f8495c5    NaN    NaN    NaN     NaN       0\n",
            "4  00436515-870c-4b36-a041-de91049b9ab4  264.0  152.0  213.0   379.0       1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Mzyh5TSe_qBt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_train = df_train_labels.drop_duplicates('patientId')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FdT3iXstvNa5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# empty dictionary\n",
        "pneumonia_locations = {}\n",
        "# load table\n",
        "with open(os.path.join('/content/Pneumonia-Detection/stage_2_train_labels.csv'), mode='r') as infile:\n",
        "    # open reader\n",
        "    reader = csv.reader(infile)\n",
        "    # skip header\n",
        "    next(reader, None)\n",
        "    # loop through rows\n",
        "    for rows in reader:\n",
        "        # retrieve information\n",
        "        filename = rows[0]\n",
        "        #print(filename)\n",
        "        if os.path.isfile('/content/Pneumonia-Detection/train/'+filename+'.dcm'):\n",
        "          #print(\"file is present \", filename)\n",
        "          location = rows[1:5]\n",
        "          pneumonia = rows[5]\n",
        "          # if row contains pneumonia add label to dictionary\n",
        "          # which contains a list of pneumonia locations per filename\n",
        "          if pneumonia == '1':\n",
        "              # convert string to float to int\n",
        "              location = [int(float(i)) for i in location]\n",
        "              # save pneumonia location in dictionary\n",
        "              if filename in pneumonia_locations:\n",
        "                  pneumonia_locations[filename].append(location)\n",
        "              else:\n",
        "                  pneumonia_locations[filename] = [location]\n",
        "        else:\n",
        "          print(\"file not present \", filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0gpvMiCNeoLl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f539e093-8d61-4fc5-ccba-6bc79e3fad82"
      },
      "cell_type": "code",
      "source": [
        "print(len(filenames))"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EDzTfah7gLA_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "085e5ba9-87bf-4c43-cea9-1846fb4f9e42"
      },
      "cell_type": "code",
      "source": [
        "print(len(pneumonia_locations))"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6012\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xAAQLAIKwnaJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7184b7f0-0a88-4d00-c2d7-8983a97dc380"
      },
      "cell_type": "code",
      "source": [
        "# load and shuffle filenames\n",
        "folder = '/content/Pneumonia-Detection/train'\n",
        "filenames = os.listdir(folder)\n",
        "random.shuffle(filenames)\n",
        "print(len(filenames))"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kTPLlZYhUTsF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b7af4d9e-b5a3-41c5-b38c-f01a674b1eb4"
      },
      "cell_type": "code",
      "source": [
        "# split into train and validation filenames\n",
        "n_valid_samples = 2560\n",
        "train_filenames = filenames[n_valid_samples:]\n",
        "valid_filenames = filenames[:n_valid_samples]\n",
        "print('n train samples', len(train_filenames))\n",
        "print('n valid samples', len(valid_filenames))\n",
        "n_train_samples = len(filenames) - n_valid_samples"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n train samples 24124\n",
            "n valid samples 2560\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kdb8E5zWU8Pl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "outputId": "a664ecfd-6fd2-4329-b9b8-f461f4007550"
      },
      "cell_type": "code",
      "source": [
        "print('Total train images:',len(filenames))\n",
        "print('Images with pneumonia:', len(pneumonia_locations))\n",
        "\n",
        "ns = [len(value) for value in pneumonia_locations.values()]\n",
        "plt.figure()\n",
        "plt.hist(ns)\n",
        "plt.xlabel('Pneumonia per image')\n",
        "plt.xticks(range(1, np.max(ns)+1))\n",
        "plt.show()\n",
        "\n",
        "heatmap = np.zeros((1024, 1024))\n",
        "ws = []\n",
        "hs = []\n",
        "for values in pneumonia_locations.values():\n",
        "    for value in values:\n",
        "        x, y, w, h = value\n",
        "        heatmap[y:y+h, x:x+w] += 1\n",
        "        ws.append(w)\n",
        "        hs.append(h)\n",
        "plt.figure()\n",
        "plt.title('Pneumonia location heatmap')\n",
        "plt.imshow(heatmap)\n",
        "plt.figure()\n",
        "plt.title('Pneumonia height lengths')\n",
        "plt.hist(hs, bins=np.linspace(0,1000,50))\n",
        "plt.show()\n",
        "plt.figure()\n",
        "plt.title('Pneumonia width lengths')\n",
        "plt.hist(ws, bins=np.linspace(0,1000,50))\n",
        "plt.show()\n",
        "print('Minimum pneumonia height:', np.min(hs))\n",
        "print('Minimum pneumonia width: ', np.min(ws))"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total train images: 26684\n",
            "Images with pneumonia: 6012\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAFYCAYAAACcb79EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAMBghBoAAIP9Dw3hVc1qFK27AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-102-14a86bf36b4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mheatmap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mhs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "I3IciamHUTi8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class generator(keras.utils.Sequence):\n",
        "    \n",
        "    def __init__(self, folder, filenames, pneumonia_locations=None, batch_size=32, image_size=256, shuffle=True, augment=False, predict=False):\n",
        "        self.folder = folder\n",
        "        self.filenames = filenames\n",
        "        self.pneumonia_locations = pneumonia_locations\n",
        "        self.batch_size = batch_size\n",
        "        self.image_size = image_size\n",
        "        self.shuffle = shuffle\n",
        "        self.augment = augment\n",
        "        self.predict = predict\n",
        "        self.on_epoch_end()\n",
        "        \n",
        "    def __load__(self, filename):\n",
        "        # load dicom file as numpy array\n",
        "        img = pydicom.dcmread(os.path.join(self.folder, filename)).pixel_array\n",
        "        # create empty mask\n",
        "        msk = np.zeros(img.shape)\n",
        "        # get filename without extension\n",
        "        filename = filename.split('.')[0]\n",
        "        # if image contains pneumonia\n",
        "        if filename in self.pneumonia_locations:\n",
        "            # loop through pneumonia\n",
        "            for location in self.pneumonia_locations[filename]:\n",
        "                # add 1's at the location of the pneumonia\n",
        "                x, y, w, h = location\n",
        "                msk[y:y+h, x:x+w] = 1\n",
        "        # resize both image and mask\n",
        "        img = resize(img, (self.image_size, self.image_size), mode='reflect')\n",
        "        msk = resize(msk, (self.image_size, self.image_size), mode='reflect') > 0.5\n",
        "        # if augment then horizontal flip half the time\n",
        "        if self.augment and random.random() > 0.5:\n",
        "            img = np.fliplr(img)\n",
        "            msk = np.fliplr(msk)\n",
        "        # add trailing channel dimension\n",
        "        img = np.expand_dims(img, -1)\n",
        "        msk = np.expand_dims(msk, -1)\n",
        "        return img, msk\n",
        "    \n",
        "    def __loadpredict__(self, filename):\n",
        "        # load dicom file as numpy array\n",
        "        img = pydicom.dcmread(os.path.join(self.folder, filename)).pixel_array\n",
        "        # resize image\n",
        "        img = resize(img, (self.image_size, self.image_size), mode='reflect')\n",
        "        # add trailing channel dimension\n",
        "        img = np.expand_dims(img, -1)\n",
        "        return img\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        # select batch\n",
        "        filenames = self.filenames[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        # predict mode: return images and filenames\n",
        "        if self.predict:\n",
        "            # load files\n",
        "            imgs = [self.__loadpredict__(filename) for filename in filenames]\n",
        "            # create numpy batch\n",
        "            imgs = np.array(imgs)\n",
        "            return imgs, filenames\n",
        "        # train mode: return images and masks\n",
        "        else:\n",
        "            # load files\n",
        "            items = [self.__load__(filename) for filename in filenames]\n",
        "            # unzip images and masks\n",
        "            imgs, msks = zip(*items)\n",
        "            # create numpy batch\n",
        "            imgs = np.array(imgs)\n",
        "            msks = np.array(msks)\n",
        "            return imgs, msks\n",
        "        \n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            random.shuffle(self.filenames)\n",
        "        \n",
        "    def __len__(self):\n",
        "        if self.predict:\n",
        "            # return everything\n",
        "            return int(np.ceil(len(self.filenames) / self.batch_size))\n",
        "        else:\n",
        "            # return full batches only\n",
        "            return int(len(self.filenames) / self.batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OTI-agmwUTZu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_downsample(channels, inputs):\n",
        "    x = keras.layers.BatchNormalization(momentum=0.9)(inputs)\n",
        "    x = keras.layers.LeakyReLU(0)(x)\n",
        "    x = keras.layers.Conv2D(channels, 1, padding='same', use_bias=False)(x)\n",
        "    x = keras.layers.MaxPool2D(2)(x)\n",
        "    return x\n",
        "\n",
        "def create_resblock(channels, inputs):\n",
        "    x = keras.layers.BatchNormalization(momentum=0.9)(inputs)\n",
        "    x = keras.layers.LeakyReLU(0)(x)\n",
        "    x = keras.layers.Conv2D(channels, 3, padding='same', use_bias=False)(x)\n",
        "    x = keras.layers.BatchNormalization(momentum=0.9)(x)\n",
        "    x = keras.layers.LeakyReLU(0)(x)\n",
        "    x = keras.layers.Conv2D(channels, 3, padding='same', use_bias=False)(x)\n",
        "    return keras.layers.add([x, inputs])\n",
        "\n",
        "def create_network(input_size, channels, n_blocks=2, depth=4):\n",
        "    # input\n",
        "    inputs = keras.Input(shape=(input_size, input_size, 1))\n",
        "    x = keras.layers.Conv2D(channels, 3, padding='same', use_bias=False)(inputs)\n",
        "    # residual blocks\n",
        "    for d in range(depth):\n",
        "        channels = channels * 2\n",
        "        x = create_downsample(channels, x)\n",
        "        for b in range(n_blocks):\n",
        "            x = create_resblock(channels, x)\n",
        "    # output\n",
        "    x = keras.layers.BatchNormalization(momentum=0.9)(x)\n",
        "    x = keras.layers.LeakyReLU(0)(x)\n",
        "    x = keras.layers.Conv2D(1, 1, activation='sigmoid')(x)\n",
        "    outputs = keras.layers.UpSampling2D(2**depth)(x)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9V2H8jNbXr-B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.path.isfile('/content/train/58f625bb-b656-4b17-b1d5-dd42de880c1f')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ApuYhmqYUTP7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define iou or jaccard loss function\n",
        "def iou_loss(y_true, y_pred):\n",
        "    y_true = tf.reshape(y_true, [-1])\n",
        "    y_pred = tf.reshape(y_pred, [-1])\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    score = (intersection + 1.) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection + 1.)\n",
        "    return 1 - score\n",
        "\n",
        "# combine bce loss and iou loss\n",
        "def iou_bce_loss(y_true, y_pred):\n",
        "    return 0.5 * keras.losses.binary_crossentropy(y_true, y_pred) + 0.5 * iou_loss(y_true, y_pred)\n",
        "\n",
        "# mean iou as a metric\n",
        "def mean_iou(y_true, y_pred):\n",
        "    y_pred = tf.round(y_pred)\n",
        "    intersect = tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3])\n",
        "    union = tf.reduce_sum(y_true, axis=[1, 2, 3]) + tf.reduce_sum(y_pred, axis=[1, 2, 3])\n",
        "    smooth = tf.ones(tf.shape(intersect))\n",
        "    return tf.reduce_mean((intersect + smooth) / (union - intersect + smooth))\n",
        "\n",
        "# create network and compiler\n",
        "model = create_network(input_size=256, channels=32, n_blocks=2, depth=4)\n",
        "model.compile(optimizer='adam',\n",
        "              loss=iou_bce_loss,\n",
        "              metrics=['accuracy', mean_iou])\n",
        "\n",
        "# cosine learning rate annealing\n",
        "def cosine_annealing(x):\n",
        "    lr = 0.001\n",
        "    epochs = 25\n",
        "    return lr*(np.cos(np.pi*x/epochs)+1.)/2\n",
        "learning_rate = tf.keras.callbacks.LearningRateScheduler(cosine_annealing)\n",
        "\n",
        "# create train and validation generators\n",
        "folder = '/content/train'\n",
        "train_gen = generator(folder, train_filenames, pneumonia_locations, batch_size=32, image_size=256, shuffle=True, augment=True, predict=False)\n",
        "valid_gen = generator(folder, valid_filenames, pneumonia_locations, batch_size=32, image_size=256, shuffle=False, predict=False)\n",
        "\n",
        "history = model.fit_generator(train_gen, validation_data=valid_gen, callbacks=[learning_rate], epochs=25, workers=4, use_multiprocessing=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "18WXm-kvUTDg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,4))\n",
        "plt.subplot(131)\n",
        "plt.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n",
        "plt.plot(history.epoch, history.history[\"val_loss\"], label=\"Valid loss\")\n",
        "plt.legend()\n",
        "plt.subplot(132)\n",
        "plt.plot(history.epoch, history.history[\"acc\"], label=\"Train accuracy\")\n",
        "plt.plot(history.epoch, history.history[\"val_acc\"], label=\"Valid accuracy\")\n",
        "plt.legend()\n",
        "plt.subplot(133)\n",
        "plt.plot(history.epoch, history.history[\"mean_iou\"], label=\"Train iou\")\n",
        "plt.plot(history.epoch, history.history[\"val_mean_iou\"], label=\"Valid iou\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bkxZJ-3uUS0O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NcW8mWeRUSkO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TkVZdRh8WFkI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YUXOOVe1WFZV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hjWyyYhFWFOH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B8m-JHs9WE-x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eBXyjtrhB-QX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir '/content/Histopathologic-Cancer-Detection'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1rk6WHL0CGfi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if not os.path.isdir('/content/Histopathologic-Cancer-Detection/test'):\n",
        "  !mkdir '/content/Histopathologic-Cancer-Detection/test'\n",
        "  \n",
        "if not os.path.isdir('/content/Histopathologic-Cancer-Detection/train'):\n",
        "  !mkdir '/content/Histopathologic-Cancer-Detection/train'  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uYh3dkXAFRzA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To copy file into colab from GCD"
      ]
    },
    {
      "metadata": {
        "id": "6UZhjDONCOWw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if not os.path.isfile('/content/Histopathologic-Cancer-Detection/test.zip'):\n",
        "  !cp '/content/drive/My Drive/All ML Datasets/Histopathologic-Cancer-Detection/test.zip' '/content/Histopathologic-Cancer-Detection'\n",
        "else:\n",
        "  print('file exists')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KkNdgVAFCxwT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import glob\n",
        "HCDBaseDirTestDir_files = sorted(glob.glob('/content/Histopathologic-Cancer-Detection/test/*'))\n",
        "#archived_files = [f for f in ipynb_files if int(f.split('/')[-1][:2]) % 2]\n",
        "archived_files = [f for f in HCDBaseDirTestDir_files]\n",
        "print(len(archived_files))\n",
        "if len(archived_files) == 0:\n",
        "  !unzip \"/content/Histopathologic-Cancer-Detection/test.zip\" -d \"/content/Histopathologic-Cancer-Detection/test/\"\n",
        "  print(len(archived_files) + ' files unzipped')\n",
        "else:\n",
        "  print('already unzipped')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eYbFQ5wz8O4H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "HCDBaseDirTestDir_files = sorted(glob.glob('/content/Histopathologic-Cancer-Detection/test/*'))\n",
        "#archived_files = [f for f in ipynb_files if int(f.split('/')[-1][:2]) % 2]\n",
        "archived_files = [f for f in HCDBaseDirTestDir_files]\n",
        "print(len(archived_files))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HlccqHaQpZJS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!kaggle competitions files -c histopathologic-cancer-detection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eDE--qt0FLUw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp '/content/drive/My Drive/All ML Datasets/Histopathologic-Cancer-Detection/model.h5' '/content/Histopathologic-Cancer-Detection/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i17v-gxzOiFF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Download competition files to colab folders"
      ]
    },
    {
      "metadata": {
        "id": "VofS_vnzs5ZU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if not os.path.isfile(\"/content/Histopathologic-Cancer-Detection/train_labels.csv.zip\"):\n",
        "  !kaggle competitions download -c histopathologic-cancer-detection -f sample_submission.csv -p \"/content/Histopathologic-Cancer-Detection/\"\n",
        "  !kaggle competitions download -c histopathologic-cancer-detection -f train_labels.csv -p \"/content/Histopathologic-Cancer-Detection/\"\n",
        "  !kaggle competitions download -c histopathologic-cancer-detection -f test.zip -p \"/content/Histopathologic-Cancer-Detection/\"\n",
        "  !kaggle competitions download -c histopathologic-cancer-detection -f train.zip -p \"/content/Histopathologic-Cancer-Detection/\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kbl9P1_EP4E1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if not os.path.isfile(\"/content/Histopathologic-Cancer-Detection/train_labels.csv\"):\n",
        "  !unzip \"/content/Histopathologic-Cancer-Detection/train_labels.csv.zip\" -d \"/content/Histopathologic-Cancer-Detection/\"\n",
        "  \n",
        "if not os.path.isfile(\"/content/Histopathologic-Cancer-Detection/sample_submission.csv\"):\n",
        "  !unzip \"/content/Histopathologic-Cancer-Detection/sample_submission.csv.zip\" -d \"/content/Histopathologic-Cancer-Detection/\"  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C5-bdIevPrSu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "HCDTrainDir_files = sorted(glob.glob('/content/Histopathologic-Cancer-Detection/train/*'))\n",
        "archived_files = 0\n",
        "archived_files = [f for f in HCDTrainDir_files]\n",
        "print(len(archived_files))\n",
        "if len(archived_files) == 0:\n",
        "  !unzip \"/content/Histopathologic-Cancer-Detection/train.zip\" -d \"/content/Histopathologic-Cancer-Detection/train/\"\n",
        "else:\n",
        "  print('already unzipped')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-XaIGdZrxQcf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if not os.path.isfile(\"drive/My Drive/All ML Datasets/Histopathologic-Cancer-Detection/train_labels.csv\"):\n",
        "  !unzip \"drive/My Drive/All ML Datasets/Histopathologic-Cancer-Detection/train_labels.csv.zip\" -d \"drive/My Drive/All ML Datasets/Histopathologic-Cancer-Detection\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oGaa5VPSlzmy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UyhP6hulGkQf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#GoogleDriveHandler?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w_fGiy0QHFQA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gcd_ID = drive_handler.path_to_id('/content/drive/My Drive/All ML Datasets/Histopathologic-Cancer-Detection/')\n",
        "gcd_ID\n",
        "#drive_handler.list_folder(gcd_ID)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E2Nwr2htcioi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.path.isdir('/content/drive/My Drive/All ML Datasets/Histopathologic-Cancer-Detection/base_dir/train_dir/a_no_tumor_tissue/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jQK6aofxs3q4",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "#import glob\n",
        "HCDBaseDirTrainDir_files = sorted(glob.glob('/content/drive/My Drive/All ML Datasets/Histopathologic-Cancer-Detection/base_dir/train_dir/a_no_tumor_tissue/*'))\n",
        "#archived_files = [f for f in ipynb_files if int(f.split('/')[-1][:2]) % 2]\n",
        "archived_files = [f for f in HCDBaseDirTrainDir_files]\n",
        "print(len(archived_files))\n",
        "\n",
        "\n",
        "#for f in archived_files:\n",
        "#    print(f)\n",
        "\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "erjV7py5szmR",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!cp -r '/content/drive/My Drive/All ML Datasets/Histopathologic-Cancer-Detection/base_dir' '/content/Histopathologic-Cancer-Detection/base_dir'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IzaBFz80LBBQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#tar_file_path = create_archive('HCDBaseDirTrainDir', local_file_paths=archived_files[:10], verbose=True)\n",
        "#tar_file_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XZFSmB4BGtU8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#print([x for x in dir(GoogleDriveHandler)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Eb1RH2COtRTY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Example of how to archive files in notebook"
      ]
    },
    {
      "metadata": {
        "id": "FaFEMFw2Y4mR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "ipynb_files = sorted(glob.glob('sample_data/*.csv'))\n",
        "#archived_files = [f for f in ipynb_files if int(f.split('/')[-1][:2]) % 2]\n",
        "archived_files = [f for f in ipynb_files]\n",
        "for f in archived_files:\n",
        "    print(f)\n",
        "    \n",
        "'''    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XcddNqKVZfuS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#tar_file_path = create_archive('sample_archive', local_file_paths=archived_files, verbose=True)\n",
        "#tar_file_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mNY-dRYAaMvH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!ls -l '/tmp/sample_archive.tar.gz'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1njth6sWtuYd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Example of how to create folders in GCD using drive handler"
      ]
    },
    {
      "metadata": {
        "id": "NbtRPdTJWSlW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "test_folder_id = drive_handler.create_folder('test_folder')\n",
        "test_folder_id\n",
        "\n",
        "test_subfolder_id = drive_handler.create_folder('test_sub_folder', parent_path='test_folder')\n",
        "test_subfolder_id\n",
        "\n",
        "same_subfolder_id = drive_handler.create_folder('test_sub_folder', parent_path='test_folder')\n",
        "test_subfolder_id\n",
        "\n",
        "same_subfolder_id2 = drive_handler.create_folder('test_sub_folder2', parent_path='test_folder')\n",
        "test_subsubfolder_id = drive_handler.create_folder('test_sub_sub_folder', parent_path='test_folder/test_sub_folder2')\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tAneW8j3uNMO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Examples of how to access list of files in a folder using drive handler"
      ]
    },
    {
      "metadata": {
        "id": "3quUhR4HW_fh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "ID = drive_handler.path_to_id('test_folder/test_sub_folder2/test_sub_sub_folder')\n",
        "ID, test_subsubfolder_id\n",
        "\n",
        "drive_handler.list_folder(test_folder_id)\n",
        "drive_handler.list_folder(test_folder_id, max_depth=1)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "caem35IcuxuR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To upload file to GCD"
      ]
    },
    {
      "metadata": {
        "id": "lWsdFCaOXEO0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#drive_handler.upload(tar_file_path, parent_path='test_folder/test_sub_folder2')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uZ1v1vJ_u6Ob",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To download file into Colab from GCD"
      ]
    },
    {
      "metadata": {
        "id": "PAsayaIzbc_-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#drive_handler.download('/tmp/downloaded_archive.tar.gz', target_path='test_folder/test_sub_folder2/sample_archive.tar.gz')\n",
        "#!ls -l '/tmp/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lwrXvcWV9OA9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "ipynb_files = sorted(glob.glob('sample_data/*.csv'))\n",
        "#archived_files = [f for f in ipynb_files if int(f.split('/')[-1][:2]) % 2]\n",
        "archived_files = [f for f in ipynb_files]\n",
        "for f in archived_files:\n",
        "    print(f)\n",
        "\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D7dgMvZ_Tguw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# PyDrive reference:\n",
        "# https://gsuitedevs.github.io/PyDrive/docs/build/html/index.html\n",
        "\n",
        "# 2. Create & upload a file text file.\n",
        "uploaded = drive.CreateFile({'title': 'Sample upload.txt'})\n",
        "uploaded.SetContentString('Sample upload file content')\n",
        "uploaded.Upload()\n",
        "print('Uploaded file with ID {}'.format(uploaded.get('id')))\n",
        "\n",
        "# 3. Load a file by ID and print its contents.\n",
        "downloaded = drive.CreateFile({'id': uploaded.get('id')})\n",
        "print('Downloaded content \"{}\"'.format(downloaded.GetContentString()))\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mcs11Y2hj2ou",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from numpy.random import seed\n",
        "seed(101)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(101)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import itertools\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cgoq9Msnj7RU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = 96\n",
        "IMAGE_CHANNELS = 3\n",
        "\n",
        "SAMPLE_SIZE = 80000 # the number of images we use from each of the two classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2XbwOmE6xIMh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Give read write access to train_labels.csv"
      ]
    },
    {
      "metadata": {
        "id": "eRxl9YPqw1EV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls -l '/content/Histopathologic-Cancer-Detection/train_labels.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-9uxNwsTmec9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!chmod 600 '/content/Histopathologic-Cancer-Detection/train_labels.csv'\n",
        "!ls -l '/content/Histopathologic-Cancer-Detection/train_labels.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jdJrIJy9kWRY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_data = pd.read_csv('/content/Histopathologic-Cancer-Detection/train_labels.csv', engine='python')\n",
        "\n",
        "# removing this image because it caused a training error previously\n",
        "df_data[df_data['id'] != 'dd6dfed324f9fcb6f93f46f32fc800f2ec196be2']\n",
        "\n",
        "# removing this image because it's black\n",
        "df_data[df_data['id'] != '9369c7278ec8bcc6c880d99194de09fc2bd4efbe']\n",
        "\n",
        "\n",
        "print(df_data.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hbJNi6N5nDNv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_data['label'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5uyCG8j1n-a4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# source: https://www.kaggle.com/gpreda/honey-bee-subspecies-classification\n",
        "\n",
        "def draw_category_images(col_name,figure_cols, df, IMAGE_PATH):\n",
        "    \n",
        "    \"\"\"\n",
        "    Give a column in a dataframe,\n",
        "    this function takes a sample of each class and displays that\n",
        "    sample on one row. The sample size is the same as figure_cols which\n",
        "    is the number of columns in the figure.\n",
        "    Because this function takes a random sample, each time the function is run it\n",
        "    displays different images.\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "    categories = (df.groupby([col_name])[col_name].nunique()).index\n",
        "    f, ax = plt.subplots(nrows=len(categories),ncols=figure_cols, \n",
        "                         figsize=(4*figure_cols,4*len(categories))) # adjust size here\n",
        "    # draw a number of images for each location\n",
        "    for i, cat in enumerate(categories):\n",
        "        sample = df[df[col_name]==cat].sample(figure_cols) # figure_cols is also the sample size\n",
        "        for j in range(0,figure_cols):\n",
        "            file=IMAGE_PATH + sample.iloc[j]['id'] + '.tif'\n",
        "            #print(file)\n",
        "            im=cv2.imread(file)\n",
        "            #print(im)\n",
        "            #print(im.shape)\n",
        "            ax[i, j].imshow(im, resample=True, cmap='gray')\n",
        "            ax[i, j].set_title(cat, fontsize=16)  \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M0bifBiHqKln",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# below is not needed. This is a high cost operation. This cell was used when the dataset was stored in GCD\n",
        "#import glob\n",
        "#train_files = glob.glob('/content/Histopathologic-Cancer-Detection/train/*')\n",
        "#print(len(train_files))\n",
        "#df_train_files = pd.DataFrame(train_files)\n",
        "#df_train_files.columns = ['filename']\n",
        "#df_train_files['tmp_col'] = 0\n",
        "#df_data_new = df_data.loc[df['id'].isin(df_train_files)]\n",
        "#df_train_files_tmp = df_train_files['filename'].split(\".\")\n",
        "#df_train_files_tmp = pd.DataFrame(df_train_files['filename'].apply(lambda x: x.split('.',1)))\n",
        "#df_train_files_tmp1 = [x[0] for x in df_train_files_tmp.filename[:]]\n",
        "#print(type(df_train_files_tmp1))\n",
        "\n",
        "#df_train_files_tmp2 = pd.DataFrame(df_train_files_tmp1) #.apply(lambda x: x.split('/',1))\n",
        "#df_train_files_tmp2.columns = ['filename']\n",
        "\n",
        "#type(df_train_files_tmp2)\n",
        "#df_train_files_tmp3 = pd.DataFrame(df_train_files_tmp2['filename'].apply(lambda x: x.split('/',-1)))\n",
        "#df_train_files_tmp4 = [x[-1] for x in df_train_files_tmp3.filename[:]]\n",
        "#df_train_files_tmp4[:10]\n",
        "#df_data_new = df_data.loc[df_data['id'].isin(df_train_files_tmp4)]\n",
        "#df_data_new.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lxupH8hnGO_q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(df_data.shape)\n",
        "print('~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
        "print(df_data.columns)\n",
        "print('~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
        "print(df_data.head())\n",
        "print('~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
        "print(df_data.id[:10])\n",
        "print('~~~~~~~~~~~~~~~~~~~~~~~~~~~')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IGBHalAdoCdy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "IMAGE_PATH = '/content/Histopathologic-Cancer-Detection/train/' \n",
        "\n",
        "draw_category_images('label',3, df_data, IMAGE_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t8hDe9nr6-BY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# take a random sample of class 0 with size equal to num samples in class 1\n",
        "df_0 = df_data[df_data['label'] == 0].sample(SAMPLE_SIZE, random_state = 101)\n",
        "# filter out class 1\n",
        "df_1 = df_data[df_data['label'] == 1].sample(SAMPLE_SIZE, random_state = 101)\n",
        "\n",
        "# concat the dataframes\n",
        "df_data_new = pd.concat([df_0, df_1], axis=0).reset_index(drop=True)\n",
        "# shuffle\n",
        "df_data_new = shuffle(df_data_new)\n",
        "\n",
        "df_data_new['label'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GYfLQRuCVqU4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# train_test_split\n",
        "\n",
        "# stratify=y creates a balanced validation set.\n",
        "y = df_data_new['label']\n",
        "\n",
        "df_train, df_val = train_test_split(df_data_new, test_size=0.10, random_state=101, stratify=y)\n",
        "\n",
        "print(df_train.shape)\n",
        "print(df_val.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AVFwHQv6Vz_M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_train['label'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uABPJFW4V5YX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_val['label'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tYXXqRre_wpE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "base_dir = '/content/Histopathologic-Cancer-Detection/base_dir'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SWCUMICRWchm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a new directory\n",
        "if not os.path.isdir('/content/Histopathologic-Cancer-Detection/base_dir'):\n",
        "  base_dir = '/content/Histopathologic-Cancer-Detection/base_dir'\n",
        "  os.mkdir(base_dir)\n",
        "else:\n",
        "  print('dir already exists')\n",
        "  data_populated = True\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IhBFYTuDV6-g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#[CREATE FOLDERS INSIDE THE BASE DIRECTORY]\n",
        "\n",
        "# now we create 2 folders inside 'base_dir':\n",
        "\n",
        "# train_dir\n",
        "    # a_no_tumor_tissue\n",
        "    # b_has_tumor_tissue\n",
        "\n",
        "# val_dir\n",
        "    # a_no_tumor_tissue\n",
        "    # b_has_tumor_tissue\n",
        "\n",
        "if not data_populated:\n",
        "  # create a path to 'base_dir' to which we will join the names of the new folders\n",
        "  # train_dir\n",
        "  train_dir = os.path.join(base_dir, 'train_dir')\n",
        "  os.mkdir(train_dir)\n",
        "\n",
        "  # val_dir\n",
        "  val_dir = os.path.join(base_dir, 'val_dir')\n",
        "  os.mkdir(val_dir)\n",
        "\n",
        "\n",
        "\n",
        "  # [CREATE FOLDERS INSIDE THE TRAIN AND VALIDATION FOLDERS]\n",
        "  # Inside each folder we create seperate folders for each class\n",
        "\n",
        "  # create new folders inside train_dir\n",
        "  no_tumor_tissue = os.path.join(train_dir, 'a_no_tumor_tissue')\n",
        "  os.mkdir(no_tumor_tissue)\n",
        "  has_tumor_tissue = os.path.join(train_dir, 'b_has_tumor_tissue')\n",
        "  os.mkdir(has_tumor_tissue)\n",
        "\n",
        "\n",
        "  # create new folders inside val_dir\n",
        "  no_tumor_tissue = os.path.join(val_dir, 'a_no_tumor_tissue')\n",
        "  os.mkdir(no_tumor_tissue)\n",
        "  has_tumor_tissue = os.path.join(val_dir, 'b_has_tumor_tissue')\n",
        "  os.mkdir(has_tumor_tissue)\n",
        "else:\n",
        "  print('sub directories already created')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "thPPqJ4KWsyb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# check that the folders have been created\n",
        "print(os.listdir(base_dir + '/train_dir'))\n",
        "print(os.listdir(base_dir + '/val_dir'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qn5p2k_raF8Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Set the id as the index in df_data\n",
        "df_data_new.set_index('id', inplace=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UjULcyeBaVxV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if not data_populated:\n",
        "\n",
        "  # Get a list of train and val images\n",
        "  train_list = list(df_train['id'])\n",
        "  val_list = list(df_val['id'])\n",
        "\n",
        "  train_src_dir = '/content/Histopathologic-Cancer-Detection/train'\n",
        "\n",
        "\n",
        "\n",
        "  # Transfer the train images\n",
        "\n",
        "  for image in train_list:\n",
        "\n",
        "      # the id in the csv file does not have the .tif extension therefore we add it here\n",
        "      fname = image + '.tif'\n",
        "      # get the label for a certain image\n",
        "      target = df_data_new.loc[image,'label']\n",
        "\n",
        "      # these must match the folder names\n",
        "      if target == 0:\n",
        "          label = 'a_no_tumor_tissue'\n",
        "      if target == 1:\n",
        "          label = 'b_has_tumor_tissue'\n",
        "\n",
        "      # source path to image\n",
        "      src = os.path.join(train_src_dir, fname)\n",
        "      # destination path to image\n",
        "      dst = os.path.join(train_dir, label, fname)\n",
        "      # copy the image from the source to the destination\n",
        "      shutil.copyfile(src, dst)\n",
        "\n",
        "\n",
        "  # Transfer the val images\n",
        "\n",
        "  for image in val_list:\n",
        "\n",
        "      # the id in the csv file does not have the .tif extension therefore we add it here\n",
        "      fname = image + '.tif'\n",
        "      # get the label for a certain image\n",
        "      target = df_data_new.loc[image,'label']\n",
        "\n",
        "      # these must match the folder names\n",
        "      if target == 0:\n",
        "          label = 'a_no_tumor_tissue'\n",
        "      if target == 1:\n",
        "          label = 'b_has_tumor_tissue'\n",
        "\n",
        "\n",
        "      # source path to image\n",
        "      src = os.path.join(train_src_dir, fname)\n",
        "      # destination path to image\n",
        "      dst = os.path.join(val_dir, label, fname)\n",
        "      # copy the image from the source to the destination\n",
        "      shutil.copyfile(src, dst)\n",
        "\n",
        "\n",
        "\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1_C7zYgkb11-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(len(os.listdir(os.path.join(base_dir, 'train_dir','a_no_tumor_tissue'))))\n",
        "print(len(os.listdir(os.path.join(base_dir, 'train_dir','b_has_tumor_tissue'))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v4ZkoZ8H_Tib",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(len(os.listdir(os.path.join(base_dir, 'val_dir','a_no_tumor_tissue'))))\n",
        "print(len(os.listdir(os.path.join(base_dir, 'val_dir','b_has_tumor_tissue'))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qwPoEeSUAaDo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_path = base_dir + '/train_dir'\n",
        "valid_path = base_dir + '/val_dir'\n",
        "test_path = '/content/Histopathologic-Cancer-Detection/test'\n",
        "\n",
        "num_train_samples = len(df_train)\n",
        "num_val_samples = len(df_val)\n",
        "train_batch_size = 10\n",
        "val_batch_size = 10\n",
        "\n",
        "\n",
        "train_steps = np.ceil(num_train_samples / train_batch_size)\n",
        "val_steps = np.ceil(num_val_samples / val_batch_size)\n",
        "\n",
        "print(num_train_samples)\n",
        "print(num_val_samples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5GOvqm_1_SS3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(rescale=1.0/255)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UrE6zgoeBAmn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "train_gen = datagen.flow_from_directory(train_path,\n",
        "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
        "                                        batch_size=train_batch_size,\n",
        "                                        class_mode='categorical')\n",
        "\n",
        "val_gen = datagen.flow_from_directory(valid_path,\n",
        "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
        "                                        batch_size=val_batch_size,\n",
        "                                        class_mode='categorical')\n",
        "\n",
        "# Note: shuffle=False causes the test dataset to not be shuffled\n",
        "test_gen = datagen.flow_from_directory(valid_path,\n",
        "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
        "                                        batch_size=1,\n",
        "                                        class_mode='categorical',\n",
        "                                        shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ibYt8xuyBPWX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "kernel_size = (3,3)\n",
        "pool_size= (2,2)\n",
        "first_filters = 32\n",
        "second_filters = 64\n",
        "third_filters = 128\n",
        "\n",
        "dropout_conv = 0.3\n",
        "dropout_dense = 0.3\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(first_filters, kernel_size, activation = 'relu', input_shape = (96, 96, 3)))\n",
        "model.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\n",
        "model.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\n",
        "model.add(MaxPooling2D(pool_size = pool_size)) \n",
        "model.add(Dropout(dropout_conv))\n",
        "\n",
        "model.add(Conv2D(second_filters, kernel_size, activation ='relu'))\n",
        "model.add(Conv2D(second_filters, kernel_size, activation ='relu'))\n",
        "model.add(Conv2D(second_filters, kernel_size, activation ='relu'))\n",
        "model.add(MaxPooling2D(pool_size = pool_size))\n",
        "model.add(Dropout(dropout_conv))\n",
        "\n",
        "model.add(Conv2D(third_filters, kernel_size, activation ='relu'))\n",
        "model.add(Conv2D(third_filters, kernel_size, activation ='relu'))\n",
        "model.add(Conv2D(third_filters, kernel_size, activation ='relu'))\n",
        "model.add(MaxPooling2D(pool_size = pool_size))\n",
        "model.add(Dropout(dropout_conv))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation = \"relu\"))\n",
        "model.add(Dropout(dropout_dense))\n",
        "model.add(Dense(2, activation = \"softmax\"))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OJjiHWcYB8RW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(Adam(lr=0.0001), loss='binary_crossentropy', \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W8mnvhfCGOA3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(val_gen.class_indices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zAelgNqtGQAs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filepath = \"model.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n",
        "                             save_best_only=True, mode='max')\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=2, \n",
        "                                   verbose=1, mode='max', min_lr=0.00001)\n",
        "                              \n",
        "                              \n",
        "callbacks_list = [checkpoint, reduce_lr]\n",
        "\n",
        "history = model.fit_generator(train_gen, steps_per_epoch=train_steps, \n",
        "                    validation_data=val_gen,\n",
        "                    validation_steps=val_steps,\n",
        "                    epochs=20, verbose=1,\n",
        "                   callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X1efdF4A2Mq7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "teTTTnip-Czw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# get the metric names so we can use evaulate_generator\n",
        "model.metrics_names"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3tNcedqk-iU3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Here the best epoch will be used.\n",
        "\n",
        "!cp 'model.h5' '/content/drive/My Drive/All ML Datasets/Histopathologic-Cancer-Detection'\n",
        "model.load_weights('/content/Histopathologic-Cancer-Detection/model.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dTrAViynpg78",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "val_loss, val_acc = \\\n",
        "model.evaluate_generator(test_gen, \n",
        "                        steps=len(df_val))\n",
        "\n",
        "print('val_loss:', val_loss)\n",
        "print('val_acc:', val_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tPfMNKyNplRC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# display the loss and accuracy curves\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Di-9FCAJqiPw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Make a prediction on the val set\n",
        "We need these predictions to calculate the AUC score, print the Confusion Matrix and calculate the F1 score."
      ]
    },
    {
      "metadata": {
        "id": "WK6qeRWCqYqu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# make a prediction\n",
        "predictions = model.predict_generator(test_gen, steps=len(df_val), verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6n0R3fnHqfsc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions.shape\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lLWw7gzsqrrL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A note on Keras class index values\n",
        "Keras assigns it's own index value (here 0 and 1) to the classes. It infers the classes based on the folder structure.\n",
        "Important: These index values may not match the index values we were given in the train_labels.csv file.\n",
        "\n",
        "I've used 'a' and 'b' folder name pre-fixes to get keras to assign index values to match what was in the train_labels.csv file - I guessed that keras is assigning the index value based on folder name alphabetical order."
      ]
    },
    {
      "metadata": {
        "id": "_XbipTCuql7n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This is how to check what index keras has internally assigned to each class. \n",
        "test_gen.class_indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s4XSDggCqxKx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Put the predictions into a dataframe.\n",
        "# The columns need to be oredered to match the output of the previous cell\n",
        "\n",
        "df_preds = pd.DataFrame(predictions, columns=['no_tumor_tissue', 'has_tumor_tissue'])\n",
        "\n",
        "df_preds.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IUwN8J1qq3L6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Get the true labels\n",
        "y_true = test_gen.classes\n",
        "\n",
        "# Get the predicted labels as probabilities\n",
        "y_pred = df_preds['has_tumor_tissue']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kDu46r42q6cs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "What is the AUC Score?\n"
      ]
    },
    {
      "metadata": {
        "id": "JATLCr_Dq7We",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "roc_auc_score(y_true, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i3XsdHzGrCZj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Create a Confusion Matrix\n"
      ]
    },
    {
      "metadata": {
        "id": "yFqRGBnoq-lr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Source: Scikit Learn website\n",
        "# http://scikit-learn.org/stable/auto_examples/\n",
        "# model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-\n",
        "# selection-plot-confusion-matrix-py\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7hULMGnqrNzQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Get the labels of the test images.\n",
        "\n",
        "test_labels = test_gen.classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NJKrlsAkrQfK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_labels.shape\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hgQMBaHXrTcW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# argmax returns the index of the max value in a row\n",
        "cm = confusion_matrix(test_labels, predictions.argmax(axis=1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nRcMPX7orW1J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Print the label associated with each class\n",
        "test_gen.class_indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RkGWJixhrara",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define the labels of the class indices. These need to match the \n",
        "# order shown above.\n",
        "cm_plot_labels = ['no_tumor_tissue', 'has_tumor_tissue']\n",
        "\n",
        "plot_confusion_matrix(cm, cm_plot_labels, title='Confusion Matrix')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FMF-DeHKrtm1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Create a Classification Report\n"
      ]
    },
    {
      "metadata": {
        "id": "1oKwRZ60rp6A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Generate a classification report\n",
        "\n",
        "# For this to work we need y_pred as binary labels not as probabilities\n",
        "y_pred_binary = predictions.argmax(axis=1)\n",
        "\n",
        "report = classification_report(y_true, y_pred_binary, target_names=cm_plot_labels)\n",
        "\n",
        "print(report)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PCoHyBP7r1nK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Recall = Given a class, will the classifier be able to detect it?\n",
        "Precision = Given a class prediction from a classifier, how likely is it to be correct?\n",
        "F1 Score = The harmonic mean of the recall and precision. Essentially, it punishes extreme values.\n",
        "\n",
        "From the confusion matrix and classification report we see that our model is equally good at detecting both classes."
      ]
    },
    {
      "metadata": {
        "id": "3cq4YaMur5rP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**MAKE A TEST SET PREDICTION**"
      ]
    },
    {
      "metadata": {
        "id": "0_0zZ5RUr3vc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Delete base_dir and it's sub folders to free up disk space.\n",
        "\n",
        "#shutil.rmtree('base_dir')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e7Gv1e6QsFVQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#[CREATE A TEST FOLDER DIRECTORY STRUCTURE]\n",
        "\n",
        "# We will be feeding test images from a folder into predict_generator().\n",
        "# Keras requires that the path should point to a folder containing images and not\n",
        "# to the images themselves. That is why we are creating a folder (test_images) \n",
        "# inside another folder (test_dir).\n",
        "\n",
        "# test_dir\n",
        "    # test_images\n",
        "\n",
        "# create test_dir\n",
        "test_dir = 'test_dir'\n",
        "os.mkdir(test_dir)\n",
        "    \n",
        "# create test_images inside test_dir\n",
        "test_images = os.path.join(test_dir, 'test_images')\n",
        "os.mkdir(test_images)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9tnBRZkTsJgh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# check that the directory we created exists\n",
        "os.listdir('test_dir')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YZ41kJ3dsMAq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Transfer the test images into image_dir\n",
        "\n",
        "test_list = os.listdir('/content/Histopathologic-Cancer-Detection/test')\n",
        "\n",
        "for image in test_list:\n",
        "    \n",
        "    fname = image\n",
        "    \n",
        "    # source path to image\n",
        "    src = os.path.join('/content/Histopathologic-Cancer-Detection/test', fname)\n",
        "    # destination path to image\n",
        "    dst = os.path.join(test_images, fname)\n",
        "    # copy the image from the source to the destination\n",
        "    shutil.copyfile(src, dst)\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-LlSnNmIsQ-W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# check that the images are now in the test_images\n",
        "# Should now be 57458 images in the test_images folder\n",
        "\n",
        "len(os.listdir('test_dir/test_images'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pRvQxDiNsWxJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Set up the generator**\n"
      ]
    },
    {
      "metadata": {
        "id": "YXx956UCsUiy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_path ='test_dir'\n",
        "\n",
        "\n",
        "# Here we change the path to point to the test_images folder.\n",
        "\n",
        "test_gen = datagen.flow_from_directory(test_path,\n",
        "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
        "                                        batch_size=1,\n",
        "                                        class_mode='categorical',\n",
        "                                        shuffle=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wSvabJrZsgio",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Make a prediction on the test images**\n"
      ]
    },
    {
      "metadata": {
        "id": "NmlpsrnTsdcH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Make a prediction on the test images\n",
        "num_test_images = 57458\n",
        "\n",
        "# make sure we are using the best epoch\n",
        "model.load_weights('/content/Histopathologic-Cancer-Detection/model.h5')\n",
        "\n",
        "predictions = model.predict_generator(test_gen, steps=num_test_images, verbose=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XYyYuv2Vslun",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Are the number of predictions correct?\n",
        "# Should be 57458.\n",
        "\n",
        "len(predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Eu-odmj9A7aY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(predictions[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-EVxttHRspMF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Put the predictions into a dataframe\n",
        "\n",
        "df_preds = pd.DataFrame(predictions, columns=['no_tumor_tissue', 'has_tumor_tissue'])\n",
        "\n",
        "df_preds.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C3jmNKCdssQl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This outputs the file names in the sequence in which \n",
        "# the generator processed the test images.\n",
        "test_filenames = test_gen.filenames\n",
        "\n",
        "# add the filenames to the dataframe\n",
        "df_preds['file_names'] = test_filenames\n",
        "\n",
        "df_preds.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CXCiE_B0tN8c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create an id column\n",
        "\n",
        "# A file name now has this format: \n",
        "# test_images/00006537328c33e284c973d7b39d340809f7271b.tif\n",
        "\n",
        "# This function will extract the id:\n",
        "# 00006537328c33e284c973d7b39d340809f7271b\n",
        "\n",
        "\n",
        "def extract_id(x):\n",
        "    \n",
        "    # split into a list\n",
        "    a = x.split('/')\n",
        "    # split into a list\n",
        "    b = a[1].split('.')\n",
        "    extracted_id = b[0]\n",
        "    \n",
        "    return extracted_id\n",
        "\n",
        "df_preds['id'] = df_preds['file_names'].apply(extract_id)\n",
        "\n",
        "df_preds.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_hplwIwitYNS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Get the predicted labels.\n",
        "# We were asked to predict a probability that the image has tumor tissue\n",
        "y_pred = df_preds['has_tumor_tissue']\n",
        "\n",
        "# get the id column\n",
        "image_id = df_preds['id']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4NWVfMgptf7t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Create a submission file**\n"
      ]
    },
    {
      "metadata": {
        "id": "2M8Zp0Abtc5A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create a submission file\n",
        "submission = pd.DataFrame({'id':image_id, \n",
        "                           'label':y_pred, \n",
        "                          }).set_index('id')\n",
        "\n",
        "submission.to_csv('patch_preds.csv', columns=['label']) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oWFwU2ZbACR1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp 'patch_preds.csv' '/content/drive/My Drive/All ML Datasets/Histopathologic-Cancer-Detection'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9e2t_GjBtln_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "submission.head()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kkf4NuyYtqEs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Delete the test_dir directory we created to prevent a Kaggle error.\n",
        "# Kaggle allows a max of 500 files to be saved.\n",
        "\n",
        "shutil.rmtree('test_dir')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8pBonbMwG18g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Below section predicts cancer probability for a given image. The image should be provided in a folder to build batch generator. The pretrained weights will be loaded in a model that is a replica of the model used for training.**"
      ]
    },
    {
      "metadata": {
        "id": "IwpxipQ-NKtz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "43CIJ4pkLsC6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create basic directory structure and get the test images to be used for prediction\n",
        "!mkdir '/content/Histopathologic-Cancer-Detection'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "356svL-sL1pe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if not os.path.isdir('/content/Histopathologic-Cancer-Detection/test'):\n",
        "  !mkdir '/content/Histopathologic-Cancer-Detection/test'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BHWcod3kL7jG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if not os.path.isfile('/content/Histopathologic-Cancer-Detection/test.zip'):\n",
        "  !cp '/content/drive/My Drive/All ML Datasets/Histopathologic-Cancer-Detection/test.zip' '/content/Histopathologic-Cancer-Detection'\n",
        "else:\n",
        "  print('file exists')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yHJY8smfNkyf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import glob\n",
        "HCDBaseDirTestDir_files = sorted(glob.glob('/content/Histopathologic-Cancer-Detection/test/*'))\n",
        "#archived_files = [f for f in ipynb_files if int(f.split('/')[-1][:2]) % 2]\n",
        "archived_files = [f for f in HCDBaseDirTestDir_files]\n",
        "print(len(archived_files))\n",
        "if len(archived_files) == 0:\n",
        "  !unzip \"/content/Histopathologic-Cancer-Detection/test.zip\" -d \"/content/Histopathologic-Cancer-Detection/test/\"\n",
        "else:\n",
        "  print('already unzipped')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "biwsu0KjJ_nr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lB6e9jjWHmbl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create replica of the model. No need to compile the model\n",
        "\n",
        "kernel_size = (3,3)\n",
        "pool_size= (2,2)\n",
        "first_filters = 32\n",
        "second_filters = 64\n",
        "third_filters = 128\n",
        "\n",
        "dropout_conv = 0.3\n",
        "dropout_dense = 0.3\n",
        "\n",
        "\n",
        "model_replica = Sequential()\n",
        "model_replica.add(Conv2D(first_filters, kernel_size, activation = 'relu', input_shape = (96, 96, 3)))\n",
        "model_replica.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\n",
        "model_replica.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\n",
        "model_replica.add(MaxPooling2D(pool_size = pool_size)) \n",
        "model_replica.add(Dropout(dropout_conv))\n",
        "\n",
        "model_replica.add(Conv2D(second_filters, kernel_size, activation ='relu'))\n",
        "model_replica.add(Conv2D(second_filters, kernel_size, activation ='relu'))\n",
        "model_replica.add(Conv2D(second_filters, kernel_size, activation ='relu'))\n",
        "model_replica.add(MaxPooling2D(pool_size = pool_size))\n",
        "model_replica.add(Dropout(dropout_conv))\n",
        "\n",
        "model_replica.add(Conv2D(third_filters, kernel_size, activation ='relu'))\n",
        "model_replica.add(Conv2D(third_filters, kernel_size, activation ='relu'))\n",
        "model_replica.add(Conv2D(third_filters, kernel_size, activation ='relu'))\n",
        "model_replica.add(MaxPooling2D(pool_size = pool_size))\n",
        "model_replica.add(Dropout(dropout_conv))\n",
        "\n",
        "model_replica.add(Flatten())\n",
        "model_replica.add(Dense(256, activation = \"relu\"))\n",
        "model_replica.add(Dropout(dropout_dense))\n",
        "model_replica.add(Dense(2, activation = \"softmax\"))\n",
        "\n",
        "model_replica.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FGRtYaXYJX1R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_replica.compile(Adam(lr=0.0001), loss='binary_crossentropy', \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bKW8cppOIT-c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp '/content/drive/My Drive/All ML Datasets/Histopathologic-Cancer-Detection/model.h5' '/content/Histopathologic-Cancer-Detection/'\n",
        "model_replica.load_weights('/content/Histopathologic-Cancer-Detection/model.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "19t2boRbCJ6P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#find full filename of a random file in the test folder\n",
        "\n",
        "filename = glob.glob('/content/Histopathologic-Cancer-Detection/test/00006537328c33e284c973d7b39d340809*')\n",
        "print(filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ms157RIkDc5C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir '/content/predict'\n",
        "!mkdir '/content/predict/pics'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f9Gbvh5oDhiz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp '/content/Histopathologic-Cancer-Detection/test/00006537328c33e284c973d7b39d340809f7271b.tif' '/content/predict/pics'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E9NtIcG-JqqT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "datagen_pred = ImageDataGenerator(rescale=1.0/255)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ancy1JL2DCm4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = 96\n",
        "test_gen_per_image = datagen_pred.flow_from_directory('/content/predict',\n",
        "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
        "                                        batch_size=1,\n",
        "                                        class_mode='categorical',\n",
        "                                        shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WD7So_aSDRrh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_replica.predict_generator(test_gen_per_image, steps=1, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dv7DcUU5EBeG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}